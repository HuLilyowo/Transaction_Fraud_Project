# Transaction Fraud Project

This project aims to capture credit card transaction fraud with a supervised model. The final model can detect about 55% of all frauds in 3% of the population. Based on the assumption — on average, $400 gain from every caught fraud and $20 loss on every false positive detection — the overall saving is over $20 million with a 3% cutoff.

### Section 1. Data Quality Report
<b>\[File: DQR - Transaction Fraud.ipynb\]</b>

There are 10 fields (2 numerical and 8 categorical) and 96,753 records. Among those, there are 3 fields with missing values or frivolous values. The overall fraudulent rate is 1.095%, which indicates the dataset is imbalanced. 

<b> Feature Description:</b>
1. Recnum - Number of records (unique index)
2. Date -  Transaction date
3. Amount - Transaction amount
4. Cardnum - Card number
5. Merchnum - Merchant number
6. Merch description - A Description of each merchant involved
7. Merch state - State where the merchant is located
8. Merch zip - The zip code of the merchant
9. Transtype - Type of transaction (we only use P - Purchase)
10. Fraud - Fraud label (flag)

### Section 2. Data Cleansing and Feature Engineering 
<b>\[File: Feature Engineering - Transaction.ipynb & De-duplication-Transaction.ipynb\]</b>

Excluded outliers and filled missing or frivolous values with the population mean or mode after thorough matching. Created 2,981 variables (after deduplication) based on the interval time between transactions, velocity, transaction amount, variability, and uniqueness, using 32 entities generated by the original 10 features.

### Section 3. Feature Selection
<b>\[File: Feature Selection - Transaction.ipynb.zip\]</b>

To reduce dimensionality, we selected variables based on a univariate Kolmogorov–Smirnov statistic. Only the top 600 variables will be kept. After that, 25 of the 600 variables will be selected using the FDR score (What fraction of all the fraud can the model catch at a particular score cutoff) by LightGBM classifier and SequentialFeatureSelector.

### Section 4. Modeling and Financial Curve
<b>\[File: Modeling & Score Cutoff - Transaction.ipynb\]</b>

Split the dataset into a training set, test set, and out-of-time set (last 3 months' records), the out-of-time set is necessary because the last 3 months' pattern is abnormal. Using logistic regression as the benchmark, several different types of models are explored, including decision tree, random forest, boosting trees, and neural network. The final model is chosen mainly based on the performance on the out-of-time set, a LightGBM model is chosen. 

Recommended cutoff of 3% for balanced sensitivity and high overall savings ($20,472,000), considering the assumption that $400 gain per caught fraud and a $20 loss for false positives.

<img width="452" alt="image" src="https://github.com/HuLilyowo/Transaction_Fraud_Project/assets/133606096/50d1f292-13e1-4245-a2d0-28d5de62a60b">




